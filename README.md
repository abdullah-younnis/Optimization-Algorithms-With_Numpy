# Optimization Algorithms

Optimization algorithms are methods used to find the optimal solution to a problem. In the context of machine learning, optimization algorithms are used to adjust the parameters of a model so that it can learn from data and make accurate predictions.

The goal of optimization algorithms is to minimize or maximize an objective function. For example, in the case of linear regression, the objective function is the mean squared error between the predicted and actual values of the target variable. Optimization algorithms adjust the parameters of the model to minimize this objective function.

There are many types of optimization algorithms, each with its own strengths and weaknesses. 
Here's which Optimization algorithms we will use and in each one I'll include their strength and weakness and describtion of my implmentation using only numpy

* **Stochastic gradient descent**: a variant of gradient descent that uses a randomly selected subset of the training data to compute the gradient at each iteration.

* **Gradient descent with momentum**: is an optimization algorithm that is commonly used to train machine learning models. It is an extension of the standard gradient descent algorithm that uses a momentum term to accelerate the learning process.

* **Adagrad**: another adaptive optimization algorithm that adjusts the learning rate for each parameter based on the historical gradients.

* **RMSProp**: a variant of Adagrad that uses a moving average of the squared gradients to adjust the learning rate.

* **Adam (Adaptive Moment Estimation)**: is a popular optimization algorithm used to train machine learning models. It is an adaptive learning rate optimization algorithm that combines ideas from both momentum-based methods and adaptive learning rate methods.

These optimization algorithms are used to train a wide variety of machine learning models, neural networks, Deep Learning and many others.
